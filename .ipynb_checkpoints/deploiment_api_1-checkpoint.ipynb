{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a78458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des packages de base :\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "#import package nécessaire au prétraitement de texte :\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "#import des packages pour la prédiction :\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#import modeles : \n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "#import embedding : \n",
    "import tensorflow_hub as hub \n",
    "import pickle\n",
    "\n",
    "#import package mise en page : \n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f023f",
   "metadata": {},
   "source": [
    "# Organisation du notebook : \n",
    "\n",
    "### 1. Déclaration des fonctions nécessaire à notre modèle de prédiction\n",
    "### 2. Mise en page de l'API avec le module Streamlite\n",
    "### 3. Zone de test pour vérifier le bon fonctionnement de l'application\n",
    "\n",
    "Seul les 1. et 2. seront transférés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928b4ab",
   "metadata": {},
   "source": [
    "# 1. Déclaration des fonctions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10d685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Fonctions prétraitement de texte  : \n",
    "\n",
    "########### Fonction 1 : ###########\n",
    "####################################\n",
    "\n",
    "#fonction suppression des balises html : \n",
    "\n",
    "def clean_balise(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text\n",
    "    \n",
    "########### Fonction 2 : ###########\n",
    "####################################\n",
    "\n",
    "\n",
    "def preprocessing(txt, list_rare_words = None,\n",
    "                  format_txt=False):\n",
    "\n",
    "    \"\"\"\n",
    "    txt : contient le document au format str qui subira le preprocessing\n",
    "    format_txt : Si True cela renvoie une chaine de caractère, sinon une liste\n",
    "    list_rare_words : liste de token a fournir si on souhaite les supprimer\n",
    "    \"\"\"\n",
    "    #tokenization et separation de la ponctuation\n",
    "    tokens = nltk.wordpunct_tokenize(txt)\n",
    "    \n",
    "    #suppression ponctuation\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    \n",
    "    #suppression majuscule : \n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "        \n",
    "    #suppression des chiffres : \n",
    "    tokens = [w for w in tokens if not w.isnumeric()]\n",
    "\n",
    "    \n",
    "    #suppression stopwords : \n",
    "    stopw = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [w for w in tokens if w not in stopw]\n",
    "\n",
    "    #Supprime les tokens fournie dans la liste en hyperparametres\n",
    "    if list_rare_words:\n",
    "        tokens = [w for w in tokens if w not in list_rare_words]\n",
    "\n",
    "        \n",
    "        \n",
    "    #Lemmatization des mots s'ils n'appartiennent pas a la liste toptag : \n",
    "    lemm = WordNetLemmatizer()\n",
    "    tmp_list = []\n",
    "\n",
    "    for i in tokens:\n",
    "        if i not in toptag: #si le token n'est pas dans la toptag liste alors on le lemmatize\n",
    "            tmp_list.append(lemm.lemmatize(i))\n",
    "        else: #sinon on conserve le token tel quel\n",
    "            tmp_list.append(i)\n",
    "    \n",
    "\n",
    "    #Suppression des mots token qui ne sont pas des mots dans le dictionnaire anglais \n",
    "    #OU qui ne sont pas dans la liste des top tags à conserver :\n",
    "    \n",
    "    tokens = [w for w in tmp_list if w in eng_words or w in toptag]    \n",
    "    \n",
    "    \n",
    "    if format_txt:\n",
    "        tokens = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "########### Fonction 3 : ###########\n",
    "####################################\n",
    "\n",
    "\n",
    "#fonction d'application de notre prétraitement de texte :\n",
    "def cleaning(doc):\n",
    "    new_doc = preprocessing(doc, \n",
    "                            list_rare_words = None, \n",
    "                            format_txt=True, \n",
    "                             )\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162d65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:03:09.343 INFO    absl: Using C:\\Users\\MohR9\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "#Fonction pour transformer le texte de l'utilisateur en feature compatible avec notre modèle de ML:\n",
    "###########################################################\n",
    "\n",
    "# Creation des features USE:\n",
    "# Chargement du modèle USE :\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "def feature_USE_fct(sentence):\n",
    "    import tensorflow_hub as hub\n",
    "#     import tensorflow_text as text\n",
    "\n",
    "    # Chargement du modèle USE\n",
    "#     embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "    # Traitement de la phrase\n",
    "    feat = embed([sentence])  # Passage de la phrase en tant que liste\n",
    "\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0510723",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fonction qui, à partir du texte rentré par l'utilisateur, va retourner une prédiction de tag :\n",
    "\n",
    "def applying(text):\n",
    "    text = clean_balise(text) #utilisation des fonctions de prétraitement de text\n",
    "    text = cleaning(text)\n",
    "    text = feature_USE_fct(text) # transformation du texte en feature compatible avec notre modèle de prédiction\n",
    "    text_USE = pd.DataFrame(text)\n",
    "    prediction = sgd.predict(text_USE) #prediction du texte\n",
    "    tag_pred = mlb.inverse_transform(prediction) #transformation de la target binarizée en target lisible \n",
    "    if len(tag_pred) == 0:\n",
    "        return  \"Le corps de votre texte ne contient pas de mot pertinent. Ressayez avec des termes plus techniques\"\n",
    "    else: return tag_pred #affichage des tags prédits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af723c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Chargement des fichiers : \n",
    "toptag = pickle.load(open(\"toptag.pkl\",\"rb\"))\n",
    "\n",
    "eng_words = pickle.load(open(\"eng_words\",\"rb\"))\n",
    "\n",
    "sgd = pickle.load(open(\"Model\",\"rb\"))\n",
    "\n",
    "mlb = pickle.load(open(\"mlb\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbc55d",
   "metadata": {},
   "source": [
    "# 2. Mise en page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a255c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:36:21.243 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\MohR9\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# Titre : \n",
    "st.title(\"Keyword prediction tool Stackoverflow \") \n",
    "\n",
    "\n",
    "# Données entrées par l'utilisateur :\n",
    "Title_input = st.text_input(\"Write the title of your request below\")\n",
    "input_body_utilisateurs = st.text_input(\"Enter the content of your request below \")\n",
    "\n",
    "#Réponse de notre modèle : \n",
    "\n",
    "applying(input_body_utilisateurs)\n",
    "\n",
    "### Ajout d'une image :\n",
    "image = Image.open('logo.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f932f",
   "metadata": {},
   "source": [
    " ## TEST ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae4663af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"I use to like coding with python, java or c++\"\n",
    "test2 = 'This is a test. I can program in c++, java and R'\n",
    "test3 = \"expert python could someone explain problem like collect supervisor http made different script print output bash php collect output without problem python work sh bin php php sleep echo test sh bin bash sleep echo item done sh test print output bin import time import import range time sleep print test write test print test supervisor file bash program command home user sh home user log php program command home user sh home user log python program command home user sh home user log thank much help driving crazy\"\n",
    "test4 = 'question whether use guarantee visibility field respect synchronized example following class field need declared volatile synchronized used class private double public synchronized void method double temp temp temp example using however volatile field necessary class b private final lock new private volatile double public void method lock lock try double temp temp temp finally lock know using volatile anyway likely impose performance would still like code correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d71c16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  c++', 'java', 'python')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applying(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3f994e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  c++', 'java')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applying(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10c895bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  cmd',)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applying(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acc81f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  multithreading', 'c++', 'java')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applying(test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99569e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.text(input_body_utilisateurs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
